# ğŸš€ AI Web Insights API

**AI Web Insights API** is an intelligent FastAPI-based service that **scrapes website homepages**, leverages **LLMs (like GPT-4 or Gemini)** to extract key business insights, and supports **conversational Q&A** based on the analysis.

---

## âœ¨ Features

- âœ… **Homepage Scraping**: Extracts textual content from a company's website.
- âœ… **AI-Powered Analysis**: Uses LLMs (e.g., GPT-4 or Gemini Pro) to structure and interpret website data.
- âœ… **Conversational Follow-ups**: Ask follow-up questions using stored session context.
- âœ… **Secure Access**: Token-based authentication.
- âœ… **Rate Limiting**: Prevents abuse by limiting request frequency.
- âœ… **Async Performance**: Fully async using FastAPI and HTTPX for non-blocking performance.

---

## âš™ï¸ Technologies Used

| Component       | Technology       |
|----------------|------------------|
| API Framework  | FastAPI          |
| Web Scraping   | BeautifulSoup / Playwright |
| AI Model       | Gemini |
| Cache/Storage  | Redis (Session storage) |
| Auth           | Bearer Token (JWT) |
| Rate Limiting  | Custom dependency |
| Testing        | Pytest + HTTPX    |

---

## ğŸ§  How It Works

1. You send a `POST /analyze` request with a website URL.
2. The backend:
   - Scrapes the homepage content.
   - Prompts an LLM to extract structured business insights.
   - Stores the results in Redis using a unique `session_id`.
3. You can send a `POST /converse` request with:
   - A `session_id`
   - A new user question
   - An optional conversation history
4. The API will reply using AI-generated answers contextualized to the analyzed website.

---

## â–¶ï¸ How to Run Locally

### ğŸ”§ Prerequisites

- Python 3.10+
- Redis (local)
- `.env` file with your Gemini API keys

### ğŸ›  Installation

```bash
git clone https://github.com/nvdeeps/firmable
pip install -r requirements.txt
```

### ğŸ§ª Start Redis (if not already running)

```bash
sudo systemctl start redis
```

### ğŸš€ Start the FastAPI App

```bash
uvicorn app.main:app --reload
```

### âœ… Test the API

```bash
PYTHONPATH=. pytest -s
```

---

## ğŸ“¡ API Endpoints

### ğŸ” `POST /analyze`

**Purpose**: Scrape and analyze a website.

#### Request Body
```json
{
  "url": "https://example.com",
  "questions": ["What are their main services?", "Where is the HQ located?"]
}
```

#### Response
```json
{
  "url": "https://example.com",
  "analysis_timestamp": "2025-07-01T12:00:00Z",
  "company_info": { ... },
  "extracted_answers": [ ... ],
  "session_id": "uuid-string"
}
```

---

### ğŸ’¬ `POST /converse`

**Purpose**: Ask follow-up questions based on earlier analysis.

#### Request Body
```json
{
  "session_id": "uuid-from-analyze",
  "query": "Whatâ€™s their unique selling point?",
  "conversation_history": [
    {"user": "What are their services?", "agent": "They offer cloud storage and analytics."}
  ]
}
```

#### Response
```json
{
  "url": "https://example.com",
  "user_query": "Whatâ€™s their unique selling point?",
  "agent_response": "They offer seamless integration between hardware and software.",
  "context_sources": ["homepage", "previous Q&A"]
}
```

---

## ğŸ” Authentication

All endpoints require an **Authorization header**:

```http
Authorization: Bearer <your-token>
```

---

## ğŸ“‚ Project Structure

```bash
app/
â”œâ”€â”€ main.py                # FastAPI app entry point
â”œâ”€â”€ ai.py                  # LLM integration and prompt handling
â”œâ”€â”€ auth.py                # Token verification logic
â”œâ”€â”€ models.py              # Pydantic request/response models
â”œâ”€â”€ scrapper.py            # Homepage scraping utility
â”œâ”€â”€ server.py              # Redis init & rate limiter
tests/                     # Pytest test suite
```

---

